{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Naive Bayes Text Prediction: Next Word Generator\n",
    "\n",
    "### Introduction and Theory\n",
    "\n",
    "**Naive Bayes** for text prediction is a probabilistic algorithm that uses statistical inference to estimate the likelihood of the next word in a sequence. It assumes that the probability of a word occurring depends on the word immediately preceding it (often called a \"bigram\" model in this context).\n",
    "\n",
    "In this project, we will build a model that learns from a Shakespearean text corpus to generate new text word-by-word. We will also implement **Temperature Scaling** to control the \"creativity\" or randomness of the generated output.\n",
    "\n",
    "### The Mathematical Model\n",
    "The model relies on calculating transition probabilities between words and then adjusting them to control diversity.\n",
    "\n",
    "#### Bayes' Theorem\n",
    "We use **Bayes' Theorem** to calculate the probability of a specific *Next Word* ($B$) following a *Current Word* ($A$).\n",
    "\n",
    "$$P(B|A) = \\frac{P(A|B) \\cdot P(B)}{P(A)}$$\n",
    "\n",
    "* $P(B|A)$ : The probability of the *Next Word* given the *Current Word* (Transition Probability).\n",
    "* $P(A|B)$ : The likelihood (How often the *Current Word* appears before the *Next Word*).\n",
    "* $P(B)$ : The probability of the *Next Word* appearing generally in the text.\n",
    "* $P(A)$ : The probability of the *Current Word* appearing generally in the text.\n",
    "\n",
    "#### Temperature Scaling ($T$)\n",
    "To introduce variety into the text generation, we adjust the probabilities using a **Temperature ($T$)** parameter inside a Softmax function.\n",
    "\n",
    "$$P'(w) = \\frac{e^{\\log(P(w))/T}}{\\sum_{i} e^{\\log(P(w_i))/T}}$$\n",
    "\n",
    "* $P(w)$ : The original probability of the word.\n",
    "* $T$ : Temperature parameter.\n",
    "    * **Low $T$ (< 1.0)**: \"Cools\" the distribution, making it sharper. The model becomes strict and repetitive, favoring only the most likely words.\n",
    "    * **High $T$ (> 1.0)**: \"Heats\" the distribution, making it flatter. The model becomes more creative and random, giving unlikely words a higher chance to be picked."
   ],
   "id": "49f42da1c6b4a4cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Libraries",
   "id": "eb573e11e8bd3754"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T20:52:10.021528Z",
     "start_time": "2025-11-30T20:52:10.017394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ],
   "id": "2e1eaee7a87321d6",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading Data",
   "id": "52f19263dde114a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T20:52:10.057370Z",
     "start_time": "2025-11-30T20:52:10.036300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('shakespeare.txt', sep='\\0', header=None, engine='python')\n",
    "text_data = \" \".join(df[0].astype(str).tolist())\n",
    "print(\"Successfully loaded shakespeare.txt\")"
   ],
   "id": "5328accf0bd783b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded shakespeare.txt\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocessing Data",
   "id": "cce927e075b2347d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T20:52:10.094209Z",
     "start_time": "2025-11-30T20:52:10.065773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Joining characters\n",
    "clean_text = \"\".join([char.lower() for char in text_data if char.isalpha() or char.isspace()])\n",
    "\n",
    "words = clean_text.split()\n",
    "print(f\"Total words: {len(words)}\")"
   ],
   "id": "ee4fd358b8eed516",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 25800\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Building Transition Probabilities",
   "id": "618d4810d186011c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T20:52:10.158509Z",
     "start_time": "2025-11-30T20:52:10.104439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Counting words\n",
    "word_counts = {}\n",
    "for w in words:\n",
    "    if w in word_counts:\n",
    "        word_counts[w] += 1\n",
    "    else:\n",
    "        word_counts[w] = 1\n",
    "\n",
    "total_words = len(words)\n",
    "\n",
    "# Counting bigrams\n",
    "bigram_counts = {}\n",
    "for i in range(len(words) - 1):\n",
    "    current_w = words[i]\n",
    "    next_w = words[i + 1]\n",
    "\n",
    "    if current_w not in bigram_counts:\n",
    "        bigram_counts[current_w] = {}\n",
    "\n",
    "    if next_w in bigram_counts[current_w]:\n",
    "        bigram_counts[current_w][next_w] += 1\n",
    "    else:\n",
    "        bigram_counts[current_w][next_w] = 1\n",
    "\n",
    "# Building Transition Probabilities\n",
    "transition_probs = {}\n",
    "\n",
    "for current_w, next_words_dict in bigram_counts.items():\n",
    "    transition_probs[current_w] = {}\n",
    "\n",
    "    for next_w, count in next_words_dict.items():\n",
    "        # P(B)\n",
    "        p_next = word_counts[next_w] / total_words\n",
    "\n",
    "        # P(A|B)\n",
    "        p_current_given_next = count / word_counts[next_w]\n",
    "\n",
    "        # P(A)\n",
    "        p_current = word_counts[current_w] / total_words\n",
    "\n",
    "        # Bayes' Theorem\n",
    "        bayes_prob = (p_current_given_next * p_next) / p_current\n",
    "        transition_probs[current_w][next_w] = bayes_prob\n",
    "\n",
    "print(f\"Model trained on {len(transition_probs)} unique words.\")"
   ],
   "id": "5c744a0ea042832b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained on 3756 unique words.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Text Generation",
   "id": "18ab1aa773ca11d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T20:52:10.184473Z",
     "start_time": "2025-11-30T20:52:10.171267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_sentence = \"the man said that\"\n",
    "num_words = 100\n",
    "temperature = 0.7\n",
    "output_list = input_sentence.strip().lower().split()\n",
    "current_word = output_list[-1]\n",
    "\n",
    "print(\"Generating text...\")\n",
    "\n",
    "for _ in range(num_words):\n",
    "    # Fallback if word is unknown\n",
    "    if current_word not in transition_probs:\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "        next_word = sorted_words[0][0]\n",
    "\n",
    "    else:\n",
    "        # Accessing word probabilities\n",
    "        possibilities = transition_probs[current_word]\n",
    "        candidates = list(possibilities.keys())\n",
    "        probs = np.array(list(possibilities.values()))\n",
    "\n",
    "        # Softmax adjustment\n",
    "        probs = np.power(probs, 1.0 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "\n",
    "        # Weighted selection\n",
    "        next_word = random.choices(candidates, weights=probs, k=1)[0]\n",
    "\n",
    "    output_list.append(next_word)\n",
    "    current_word = next_word"
   ],
   "id": "6e2c488fc06934e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Displaying Output",
   "id": "3a4e57a3c2115717"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T20:52:10.202916Z",
     "start_time": "2025-11-30T20:52:10.195772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generated_text = \" \".join(output_list)\n",
    "\n",
    "final_output = \"\"\n",
    "words_in_output = generated_text.split()\n",
    "for i in range(0, len(words_in_output), 10):\n",
    "    final_output += \" \".join(words_in_output[i:i+10]) + \"\\n\"\n",
    "\n",
    "print(f\"Final Output:\\n\")\n",
    "print(final_output)"
   ],
   "id": "d8d6a22bcc602830",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output:\n",
      "\n",
      "the man said that title romeo can see that tybalt\n",
      "murdered doting not sir and in all the frowning night\n",
      "romeo mercutio lets retire the wormwood on the sun o\n",
      "here and lady capulet come knock and yet i not\n",
      "a madmans mercy bid me alack alack alack my lord\n",
      "what i might live therefore women may be the cords\n",
      "that i will take it to pleading and all about\n",
      "and in this of tybalt the winds thy pains farewell\n",
      "ancient vault to in capulets house of the heavens do\n",
      "protest which you to make haste friar lawrence these sad\n",
      "burial feast tybalt mercutio\n",
      "\n"
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
